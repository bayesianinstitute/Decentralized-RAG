{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loding and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\faiza\\Music\\llmResearch\\rag\\env\\Lib\\site-packages\\pypdf\\_crypt_providers\\_cryptography.py:32: CryptographyDeprecationWarning: ARC4 has been moved to cryptography.hazmat.decrepit.ciphers.algorithms.ARC4 and will be removed from this module in 48.0.0.\n",
      "  from cryptography.hazmat.primitives.ciphers.algorithms import AES, ARC4\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import  PyPDFDirectoryLoader\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "def load_directory_pdf(directory_path):\n",
    "    loader = PyPDFDirectoryLoader(directory_path)\n",
    "    docs = loader.load()\n",
    "    return docs\n",
    "\n",
    "\n",
    "def text_splitter(document):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2000,\n",
    "        chunk_overlap=100,\n",
    "        length_function=len,\n",
    "        is_separator_regex=False,\n",
    "    )\n",
    "    texts = text_splitter.split_documents(document)\n",
    "\n",
    "    return texts\n",
    "\n",
    "import os\n",
    "os.chdir(\"../\")\n",
    "\n",
    "# Load PDF Files\n",
    "DATA_DIR = Path(\"./data\")\n",
    "data = load_directory_pdf(DATA_DIR)\n",
    "\n",
    "document=text_splitter(data)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6478316783905029, 1.3252785205841064, -3.1200063228607178, -1.623626947402954, 1.1204919815063477, 0.7267512083053589, -1.7795132398605347, 0.8745321035385132, -0.493694543838501, 0.13032908737659454, -0.10548596829175949, 0.7464886903762817, 1.5896183252334595, 0.8342281579971313, -1.5233094692230225, -1.5566062927246094, 0.7202525734901428, -0.7342134714126587, 0.4372457265853882, 1.0877854824066162, 0.014299079775810242, -0.7033194303512573, 1.310410499572754, 0.5492509007453918, 0.781649112701416, 1.08658766746521, -1.0013372898101807, 0.9904807209968567, -0.2921236455440521, 0.21465399861335754, 0.38804811239242554, -0.5687799453735352, 0.2024337351322174, -0.874481201171875, -0.3569786548614502, -0.7931259870529175, 1.5731021165847778, 0.25834956765174866, 0.9956268072128296, 1.2419655323028564, 0.5690503716468811, 0.7958667874336243, -0.8756253719329834, -0.2181982547044754, 1.0355340242385864, 0.46038001775741577, 0.17863613367080688, -0.27561068534851074, -0.3748118281364441, 0.26785093545913696, -0.8276177048683167, -0.9585875272750854, -0.374705046415329, -1.0833535194396973, 1.0160481929779053, 1.4524040222167969, 1.9758358001708984, -0.5267980098724365, -0.46163854002952576, 0.3026909828186035, 1.4236432313919067, 0.1830327957868576, 0.06413735449314117, 1.6099380254745483, 0.3631365895271301, -0.32613033056259155, -0.4528923034667969, 0.7196322083473206, -0.2708841562271118, -0.4322509467601776, 1.37263023853302, -0.8752846717834473, 0.8131456971168518, -0.1954968273639679, 0.3581964671611786, 0.23908241093158722, 0.747721791267395, -0.049874842166900635, -1.870718240737915, 0.76956707239151, 2.0200958251953125, -0.48647379875183105, 1.323630928993225, -0.22813692688941956, 0.9276705384254456, 0.10351067781448364, -0.2677314281463623, 0.2400841861963272, -0.6540557742118835, 0.8250548243522644, -0.18524731695652008, 0.7738690376281738, 1.6184197664260864, -0.050738658756017685, -0.5549003481864929, -0.0869983583688736, -0.4221380949020386, -0.43873557448387146, -0.30331388115882874, -0.9904959797859192, -0.7478642463684082, 0.36036717891693115, -0.011237896978855133, -0.6343448758125305, 0.949597179889679, 1.863477110862732, -1.2487998008728027, -0.6134289503097534, -1.256691813468933, -0.6797020435333252, -0.6860373616218567, -0.48098644614219666, -0.15607963502407074, -0.6454519033432007, -0.21283546090126038, 0.47161027789115906, 1.4646894931793213, -0.3686436712741852, 0.8403099179267883, 0.6872507333755493, -0.6071953773498535, -0.4690741300582886, -0.6308890581130981, 1.1551541090011597, 0.2964923083782196, 1.2037261724472046, -0.8140305280685425, -0.682267427444458, -0.24976606667041779, 0.39412981271743774, 0.4095475971698761, -0.015394255518913269, 0.006453527137637138, -0.4948030412197113, -0.3628906309604645, 0.9799700379371643, 0.08570674806833267, -0.9254738688468933, 1.7539660930633545, 1.2972701787948608, 1.91261887550354, -0.22711226344108582, -0.06921730935573578, -0.4906790852546692, 0.17228052020072937, -0.2065575271844864, 1.2097468376159668, -0.9528515934944153, -1.3300788402557373, 0.6502470374107361, 0.5752582550048828, 0.708038330078125, 0.06705109030008316, 1.3407776355743408, 0.4594113230705261, -0.6217785477638245, -0.33880478143692017, 1.1125211715698242, 0.22508341073989868, 0.26732462644577026, 0.9429967999458313, -0.02184271439909935, -1.1554795503616333, -0.33668941259384155, -0.36653846502304077, -0.38091638684272766, 0.6361628174781799, 0.902357280254364, 0.3706166744232178, 0.7242290377616882, -0.27620816230773926, -0.7374545335769653, -0.5385850667953491, -0.42971476912498474, 0.708384096622467, -0.34370166063308716, 0.4235720932483673, -0.7834186553955078, 0.37865737080574036, -1.0766056776046753, 0.7551357746124268, -0.5381452441215515, -0.7824525237083435, 0.8110303282737732, -0.014721140265464783, -0.22745554149150848, -1.3552191257476807, -0.9003538489341736, -0.1647750288248062, -1.0508908033370972, -0.7404502034187317, 0.6413025259971619, -0.08104506880044937, 0.20382291078567505, -0.13640092313289642, -0.8506019711494446, 1.1601983308792114, 0.7625904679298401, -0.1919906884431839, -1.0627694129943848, 0.29281583428382874, -0.09402459114789963, -1.0134141445159912, 0.6609052419662476, -1.2020379304885864, 0.1943444311618805, -0.5461313128471375, 0.26979151368141174, 0.8408184051513672, -0.19308969378471375, 1.409893274307251, 0.22734111547470093, -0.9423561096191406, 0.12869934737682343, -0.8673825263977051, -0.735446572303772, -1.0774345397949219, -0.6941816210746765, -0.7513830661773682, 0.617135226726532, 0.7643600106239319, 0.6680243015289307, 0.937759280204773, -0.896794319152832, 1.266512155532837, 0.7918267250061035, -0.08801154792308807, -0.009155578911304474, -1.6968512535095215, 0.4385080933570862, -0.5402199029922485, -0.23622211813926697, 1.3932957649230957, -0.2109120637178421, 0.548856794834137, 1.6337358951568604, 0.4829854667186737, -0.034273043274879456, 0.6377455592155457, -0.699658215045929, -0.4112822711467743, 0.6557541489601135, -0.6381701231002808, -1.0295336246490479, -0.6264135837554932, -0.2800028622150421, 0.16097398102283478, -0.8314517736434937, 0.7320814728736877, 1.100561261177063, 0.21438831090927124, -0.20379365980625153, 0.2894800901412964, -0.42992836236953735, 0.7134640216827393, -1.0794686079025269, 0.5250792503356934, -0.6860061883926392, -0.7606784701347351, -1.0812585353851318, 0.1599312126636505, -2.0872180461883545, 0.5872970223426819, -1.6699615716934204, -0.45894601941108704, -0.7094188928604126, -0.5826213359832764, -0.042795538902282715, 0.8046537637710571, -0.6007037162780762, 0.7054653167724609, 1.0435479879379272, 1.5493855476379395, 0.050153229385614395, 0.09658997505903244, 0.05595811828970909, -0.4254586398601532, -1.0835570096969604, -0.10979577153921127, 0.18701690435409546, 1.0834349393844604, -1.0882147550582886, -0.8769500255584717, 0.18525685369968414, -0.37347444891929626, 0.5719529986381531, 0.6657817363739014, 0.021013852208852768, 0.03726557642221451, 0.2987551689147949, 0.5265806317329407, 0.2953796684741974, -0.7410909533500671, 0.5307058095932007, 0.41004085540771484, -0.09793244302272797, 1.4137171506881714, 0.06565651297569275, 0.025946319103240967, -0.03474976867437363, -0.19905060529708862, -0.08524782210588455, 0.7295581102371216, 1.417500376701355, 0.6177499890327454, 0.4399178624153137, 0.9433781504631042, 0.17275850474834442, 1.5995848178863525, 0.09504540264606476, -0.46761345863342285, -0.06834651529788971, -0.10363273322582245, 0.8457202911376953, -1.812545657157898, 0.018903568387031555, 0.23331601917743683, 0.1701015830039978, 1.4528108835220337, 0.23525089025497437, 0.041397158056497574, -0.8825969696044922, -0.19172562658786774, -0.6803249716758728, 0.44575929641723633, -0.14645369350910187, -0.2462434321641922, 0.8164916634559631, 0.19512243568897247, -1.0411478281021118, -0.3509986698627472, 0.8199596405029297, -0.22299343347549438, -0.945549488067627, -0.9400475025177002, 0.1770763397216797, -0.28640618920326233, 0.5424522161483765, -0.6193250417709351, 0.7304372191429138, 1.4978876113891602, -0.46879827976226807, 0.6726109981536865, -1.2059533596038818, 1.5177364349365234, -0.1745586097240448, -0.777601420879364, 0.33102086186408997, 0.7878833413124084, -0.004722753539681435, -0.1809784471988678, -1.2243916988372803, 0.05814340338110924, 1.223740816116333, -0.18505409359931946, -0.2985873222351074, -0.15801703929901123, 0.4987567961215973, -1.0524874925613403, 0.08020838350057602, 0.055646661669015884, -0.7906597256660461, -0.5109222531318665, -0.9183967709541321, 0.3858462870121002, 0.4707518517971039, 0.9963387250900269, 0.4733748137950897, 0.6803122162818909, 0.5955373048782349, -0.06074438989162445, -1.0643762350082397, 0.08824038505554199, 0.20195797085762024, 1.0742005109786987, 0.32924747467041016, -0.9019810557365417, -0.1671827733516693, -0.41810059547424316, 0.3501761555671692, 0.7183100581169128, -0.3927771747112274, -0.38245460391044617, 0.5833129286766052, 0.924700915813446, 0.17423060536384583, -0.09587299078702927, -0.13262751698493958, -0.02499796450138092, -0.30328649282455444, 0.5862826108932495, -0.07648126035928726, -0.9786326289176941, -0.2556138336658478, 0.5587573051452637, 0.039302751421928406, 0.8019484877586365, -1.1455128192901611, -0.4400577247142792, 0.03280792012810707, 0.07506762444972992, -0.6593390703201294, 0.23749670386314392, 0.3244853615760803, -0.4994482100009918, 0.9781343340873718, 0.36379384994506836, -1.333577275276184, 0.3319171369075775, 1.012076735496521, -0.8483238816261292, 1.147282361984253, -0.2625516951084137, -1.1638189554214478, -0.7862154841423035, 0.694349467754364, 0.9395084977149963, 1.0918735265731812, 0.10589302331209183, 0.26961031556129456, 0.03907838463783264, 1.3243836164474487, -0.35571756958961487, 0.8553411960601807, -0.24294959008693695, -0.03450949490070343, -0.27572253346443176, 0.961841881275177, -0.3549242913722992, -1.510942816734314, -0.23690858483314514, -0.205856591463089, -0.32475996017456055, 0.8137050867080688, 0.6295256018638611, -0.5119599103927612, -0.30299097299575806, -0.4251844882965088, 0.816562831401825, 0.35536670684814453, 0.9927842617034912, -1.4008623361587524, -0.5284557342529297, 0.31076088547706604, -0.6960986852645874, 1.1968919038772583, 0.721666157245636, -0.13574083149433136, -1.1118500232696533, 0.17384925484657288, -1.8854444026947021, 0.8660502433776855, -0.3216019868850708, 0.44376978278160095, 0.9954131841659546, -1.0696849822998047, -0.4085356593132019, -0.47675883769989014, 0.21156039834022522, 0.5149397253990173, 0.25229841470718384, 0.02846340462565422, -1.2682347297668457, 0.5714432001113892, 0.6883898377418518, -0.17109817266464233, 0.3559921383857727, -0.3248063027858734, -0.06845963001251221, 1.0714977979660034, 0.2548714280128479, 0.9927893280982971, 0.13012570142745972, 0.1268814355134964, 0.1277865767478943, -0.8500019311904907, 0.39302635192871094, -0.6423195004463196, 0.06679175049066544, 1.1389633417129517, -0.258761465549469, -0.036183737218379974, -0.09941597282886505, -1.1421515941619873, 0.19737064838409424, 1.041290283203125, 0.2381315380334854, -0.0495586171746254, 0.04873737692832947, -0.014268256723880768, 0.35118934512138367, 0.13671079277992249, 0.4265925884246826, -0.7522424459457397, 0.3769119083881378, -1.363965392112732, -0.5531790852546692, 0.2380247563123703, 0.992097795009613, 0.4391220808029175, -0.7384207248687744, 0.6065362095832825, -0.9182327389717102, 0.6817996501922607, 0.5740314722061157, -0.27583760023117065, -1.2394434213638306, -1.6361970901489258, -1.0574402809143066, 0.09175920486450195, -1.034293293952942, -0.21273821592330933, 0.055187735706567764, 0.9693677425384521, 2.0419816970825195, -0.8095582127571106, 0.7446097731590271, -0.11433161795139313, -0.5510993003845215, 0.3826669156551361, 0.20407207310199738, -0.5726140737533569, -0.4325725734233856, -0.7918699979782104, -0.31165584921836853, -0.6055853366851807, 0.2847205102443695, -1.1684361696243286, -0.5138871073722839, -0.4779166579246521, 1.1547878980636597, -0.11144714057445526, -1.3360587358474731, -0.39663165807724, -0.4535025358200073, -0.20683832466602325, -0.3794112205505371, 0.11831572651863098, 0.9317589402198792, -0.27518609166145325, -0.8104506731033325, -0.5475855469703674, -0.6665012836456299, 0.41326218843460083, 0.07848603278398514, -0.11123482882976532, -0.7076823711395264, 1.1622934341430664, 0.3396199643611908, -0.2536815404891968, 1.0785939693450928, -0.6738924384117126, -0.4770054817199707, -0.04003405570983887, 0.38284456729888916, -0.7634650468826294, -0.9722458720207214, 0.04055812954902649, -0.4501413106918335, -0.6777405142784119, 0.5321003794670105, 0.029920265078544617, 0.9711373448371887, -0.39302635192871094, -1.4096084833145142, 1.130664587020874, -0.1963377445936203, 0.49951687455177307, 0.8299161195755005, -0.09916490316390991, 0.21467941999435425, -0.06240464746952057, -0.789185106754303, -1.0725326538085938, 0.7482405304908752, 0.026014968752861023, 0.1974869668483734, 0.7449962496757507, -0.8860852718353271, -1.7033599615097046, 1.2688144445419312, -0.1706024557352066, -0.7338651418685913, -0.41119641065597534, -0.6281221508979797, -0.8002627491950989, -1.8659179210662842, -0.3909923732280731, 0.7963804006576538, -0.05714546889066696, 0.7789947390556335, -0.019762933254241943, -0.4533957242965698, -0.2390226125717163, -0.051025956869125366, 0.5444328188896179, -0.49047571420669556, -1.3128787279129028, 0.29608550667762756, -0.711320698261261, -0.4668049216270447, -0.05465634912252426, 1.78919517993927, -0.3257044851779938, -1.1123992204666138, 0.5342424511909485, 1.061508297920227, 0.5451472997665405, -0.20070578157901764, -0.06684770435094833, 0.4940555691719055, 0.4933754801750183, -0.7778047919273376, -0.17495523393154144, 0.7547875046730042, 0.01351088285446167, 1.3925228118896484, -0.5850418210029602, -1.014522671699524, 0.4880704879760742, -0.24400347471237183, -0.855259895324707, -0.15290340781211853, 0.43379297852516174, 0.4184870421886444, -0.1815810203552246, -0.49748796224594116, 0.16670477390289307, -0.11514522135257721, 0.2716519832611084, -0.3043995201587677, 0.24809052050113678, -1.8378486633300781, -0.31695953011512756, -0.2803727984428406, 0.284613698720932, -0.7052479386329651, 0.3376733660697937, 0.25360143184661865, 0.4205414056777954, 0.38841426372528076, 0.0833587497472763, 0.08061093091964722, 0.051023103296756744, 0.3818138837814331, -0.11067548394203186, 1.6966884136199951, -0.4595816731452942, 0.5758340954780579, -1.0039255619049072, 0.5753039717674255, 1.9271212816238403, -0.32422348856925964, 0.7592393755912781, 0.5690138339996338, -0.5762447118759155, 0.773553729057312, -0.8962947130203247, -1.4220160245895386, 0.17822490632534027, 0.001805182546377182, -0.6560693979263306, -1.131722331047058, 0.7723129987716675, 1.1361360549926758, -0.3629891574382782, 0.043059952557086945, -1.4049302339553833, -1.0869741439819336, -0.17717230319976807, -0.29600411653518677, 0.27649039030075073, 0.0382612869143486, -0.7889105081558228, 0.5360158681869507, 0.8066470623016357, 0.17022106051445007, 1.7168657779693604, 0.8735087513923645, 0.6148756146430969, -0.27908629179000854, -0.49847444891929626, -0.3112388551235199, 0.6100194454193115, 0.5277743339538574, -2.2116994857788086, 0.38661858439445496, -1.1827962398529053, -0.004629909992218018, -0.9354608058929443, 0.2052651345729828, -1.54283607006073, 0.013654554262757301, -0.8671385049819946, -0.25670966506004333, -0.8966925740242004, 1.0719630718231201, 0.019351046532392502, -1.1951019763946533, 1.2874460220336914, -1.00648832321167, 0.041811585426330566, 0.890895664691925, 0.7258765697479248, -0.21682529151439667, -0.2617565393447876, 0.01306721568107605, 0.713947057723999, 0.2994568943977356, 0.3869548439979553, -0.5621846914291382, 0.39010757207870483, -0.06206522881984711, 0.7107587456703186, 0.30625906586647034, -1.1089617013931274, -0.6284297704696655, -0.10018890351057053, -0.7382077574729919, 0.9739035964012146, 0.6205861568450928, 0.18274667859077454, -0.05313846468925476, -0.45359912514686584, 0.20871978998184204, 0.26399901509284973, 1.262915849685669, -0.8594906330108643, 0.9031607508659363, 0.3156881034374237, 0.49506238102912903, -0.6808839440345764, -1.2208932638168335, 0.35353225469589233, -0.8095607757568359, -0.09258199483156204, -0.6841692328453064, -0.3933556079864502, -0.8464424014091492, -0.5663899779319763, -0.4968167245388031, -0.34483686089515686, 0.08658014982938766, 0.5719478726387024, 0.4013136923313141, -0.22435112297534943, -0.630410373210907, 0.2143183797597885, 0.7839373350143433, -0.49495306611061096, 0.15640507638454437, 0.4731002151966095, -0.21593034267425537, -0.5306728482246399, 0.42502641677856445, 0.781059205532074, -0.18138062953948975, 0.8119964599609375, 1.182206392288208, -0.6979292631149292, 0.10196738690137863, -0.5643343329429626, 1.070335865020752, 0.1550181359052658, -0.907757580280304, -0.5423340201377869, -0.7228763699531555, -0.5516231060028076]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import ollama\n",
    "# client = OpenAI(base_url=\"http://localhost:1234/v1\", api_key=\"lm-studio\")\n",
    "def get_embedding(text, model='nomic-embed-text:latest'):    \n",
    "    return ollama.embeddings(model=model, prompt=text)['embedding']\n",
    "\n",
    "print(get_embedding(\"Once upon a time, there was a cat.\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector DB Successfully created :law_docs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "from qdrant_client import QdrantClient,models\n",
    "\n",
    "\n",
    "# Qdrant\n",
    "QDRANT_HOST = \"http://localhost:6333\"  # Local Qdrant\n",
    "QDRANT_COLLECTION = \"law_docs\"\n",
    "\n",
    "\n",
    "\n",
    "qclient = QdrantClient(url=QDRANT_HOST)\n",
    "\n",
    "\n",
    "\n",
    "def createDB(collectionName):\n",
    "        if qclient.collection_exists(collection_name=f\"{collectionName}\"):\n",
    "                print(f\"Vector DB already exits :{collectionName}\")\n",
    "                return\n",
    "        \n",
    "        print(f\"Vector DB Successfully created :{collectionName}\")\n",
    "        \n",
    "        return qclient.create_collection(\n",
    "                collection_name=f\"{collectionName}\",\n",
    "                vectors_config=models.VectorParams(size=768, distance=models.Distance.COSINE),)\n",
    "\n",
    "\n",
    "createDB(QDRANT_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uuid\n",
    "def embed_and_store(chunks,QDRANT_COLLECTION):\n",
    "\n",
    "    \"\"\"Embeds text chunks and stores them in Qdrant.\"\"\"\n",
    "    for i, chunk in enumerate(chunks):\n",
    "        embedding = get_embedding(chunk.page_content)\n",
    "        # print(f\"{i} : {embedding}\")\n",
    "        qclient.upsert(\n",
    "            collection_name=QDRANT_COLLECTION,\n",
    "            points=[\n",
    "                models.PointStruct(\n",
    "                id= f'{uuid.uuid4()}', vector= embedding, payload= {\"data\":chunk.page_content,\"metadata\":chunk.metadata}),\n",
    "            ],\n",
    "        )\n",
    "\n",
    "\n",
    "embed_and_store(document,QDRANT_COLLECTION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relevant information:\n",
      "Score  : 0.60554457\n",
      "Data :\n",
      " security account number issued to an alien no t authorized to work in the United States, the \n",
      "Commissioner of Social Security shall provide the Attorney General with information regarding the name \n",
      "and address of the alien, the name and address of the person reporting the earnings, and the amount of \n",
      "the earnings. The information shall be provided in an electronic form agreed upon by the Commissioner \n",
      "and the Attorney General.  \n",
      "  \n",
      "(d) A written certification signed by the Attorney Genera l or by any officer of the Service designated by the \n",
      "Attorney General to make such certif ication, that after diligent search no record or entry of a specified \n",
      "nature is found to exist in the records of the Service,  shall be admissible as evidence in any proceeding as \n",
      "evidence that the records of the Service contain no such record or entry, and shall have the same effect as \n",
      "the testimony of a witness given in open court.  \n",
      "  \n",
      "  \n",
      "FOOTNOTES FOR SECTION 290   \n",
      "  \n",
      "INA: ACT 290 FN 1   \n",
      "  FN 1      Amended by § 414 of IIRIRA .  \n",
      " \n",
      "INA: ACT 291 - BURDEN OF PROOF  \n",
      "  \n",
      "Sec. 291. [8 U.S.C. 1361] Whenever any person makes application for a visa or any other document \n",
      "required for entry, or makes application for admission, or otherwise attempts to enter the United States, the \n",
      "burden of proof shall be upon such person to establish that he is eligible to receive such visa or such \n",
      "document, or is not inadmissible under any provision of th is Act, and, if an alien, that he is entitled to the\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "def query_documents(query):\n",
    "    \"\"\"Searches Qdrant for relevant documents.\"\"\"\n",
    "    query_embedding = get_embedding(query)\n",
    "    return  qclient.search(\n",
    "        collection_name=QDRANT_COLLECTION,\n",
    "        query_vector=query_embedding,\n",
    "        limit=1,\n",
    "    )[0]\n",
    "\n",
    "\n",
    "# Example user query\n",
    "user_query = \"How Get a Social Security Number\"\n",
    "result = query_documents(user_query)\n",
    "print(\"Relevant information:\")\n",
    "\n",
    "score=result.score\n",
    "context=result.payload['data']\n",
    "print(\"Score  :\",score)\n",
    "print(\"Data :\\n\",context)\n",
    "print(\"--\"*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "prompt_Template=f\"\"\" You are Lawyer\n",
    "\n",
    "here is the question : {user_query}\n",
    "\n",
    "and addition context to support answer  {context}\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "systemPrompt=\"You are helpful assistant that handle user query and give answer and only answer to user question and use it context rather than take information from outside system  \"\n",
    "# Point to the local server\n",
    "\n",
    "response = ollama.chat(\n",
    "    model='llama3:8b', \n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": systemPrompt},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ],\n",
    "    # stream=True\n",
    ")\n",
    "response_json=response['message']['content']\n",
    "\n",
    "# for chunk in response:\n",
    "#   print(chunk['message']['content'], end='', flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I'd be happy to help you with that!\n",
      "\n",
      "To get a Social Security number (SSN), follow these steps:\n",
      "\n",
      "1. **Determine eligibility**: You can apply for an SSN if you're a:\n",
      "\t* U.S. citizen\n",
      "\t* Lawful permanent resident (with a Green Card)\n",
      "\t* Refugee or asylum seeker\n",
      "2. **Gather required documents**:\n",
      "\t* Proof of identity: A valid government-issued ID, such as a driver's license, state ID, or passport.\n",
      "\t* Proof of age: A birth certificate or adoption papers.\n",
      "\t* Proof of immigration status (if applicable): A Green Card, I-94 arrival/departure record, or other relevant documents.\n",
      "3. **Apply for an SSN online**:\n",
      "\t* Go to the Social Security Administration's (SSA) website ([www.ssa.gov](http://www.ssa.gov)) and fill out Form SSA-1, Application for a Social Security Card.\n",
      "\t* Upload supporting documentation.\n",
      "\t* Create an account or sign in if you already have one.\n",
      "4. **Apply by phone**:\n",
      "\t* Call the SSA at 1-800-772-1213 (TTY 1-800-325-0778) and ask to apply for a Social Security number.\n",
      "5. **Visit your local SSA office**:\n",
      "\t* Make an appointment with your local SSA office.\n",
      "\t* Bring the required documents and proof of identity.\n",
      "\n",
      "**Tips and Reminders**:\n",
      "\n",
      "* Apply for an SSN only if you're authorized to work in the United States.\n",
      "* If you have a temporary or replacement Social Security card, you can use it as proof of age and citizenship until your new card arrives.\n",
      "* Keep your original documents safe, as you'll need them again if you apply for other government benefits or services.\n",
      "\n",
      "That's it! I hope this helps.\n"
     ]
    }
   ],
   "source": [
    "print(response_json) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'llama3:8b',\n",
       " 'created_at': '2024-12-03T17:43:10.9917587Z',\n",
       " 'message': {'role': 'assistant', 'content': \"Hey! How's it going?\"},\n",
       " 'done_reason': 'stop',\n",
       " 'done': True,\n",
       " 'total_duration': 2816612800,\n",
       " 'load_duration': 31942600,\n",
       " 'prompt_eval_count': 11,\n",
       " 'prompt_eval_duration': 1285708000,\n",
       " 'eval_count': 8,\n",
       " 'eval_duration': 1497236000}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from ollama import Client\n",
    "from ollama import ChatResponse\n",
    "\n",
    "client=Client(host=\"http://localhost:11434\")\n",
    "\n",
    "response:ChatResponse=client.chat(model=\"llama3:8b\",messages=[{\n",
    "    \"role\":\"user\",\n",
    "    \"content\":\"hey\",\n",
    "}])\n",
    "\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hey! How's it going?\""
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evulating with deep Evaul"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom LLM Evualtor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Coroutine\n",
    "from deepeval.models import DeepEvalBaseLLM\n",
    "from ollama import Client\n",
    "from ollama import ChatResponse\n",
    "import asyncio\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class customLMStudio(DeepEvalBaseLLM):\n",
    "    def __init__(self, url=\"http://localhost:11434\",model=\"llama3:8b\"):\n",
    "        self.model = Client(host=url)\n",
    "        self.model_name = model\n",
    "\n",
    "    def load_model(self, *args, **kwargs)->Client:\n",
    "        return self.model\n",
    "    \n",
    "    def generate(self, prompt: str) -> str:\n",
    "        client = self.load_model()\n",
    "        completion:ChatResponse = client.chat(\n",
    "            model=self.model_name,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"Your helpful AI for Evaluation\"},\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ],\n",
    "            # format=\"json\"\n",
    "        )\n",
    "        return completion['message'][\"content\"]\n",
    "    \n",
    "    async def a_generate(self, prompt: str) -> str:\n",
    "        # Use asyncio.to_thread to run the blocking generate method in a separate thread\n",
    "        return self.generate(prompt=prompt)\n",
    "\n",
    "    def get_model_name(self):\n",
    "        return self.model_name\n",
    "\n",
    "c = customLMStudio()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Hi! I'm happy to help with any questions or topics you'd like to discuss. What's on your mind today?\""
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.generate(\"Hey\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llama3:8b'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.get_model_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<coroutine object customLMStudio.a_generate at 0x000001B62663D080>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c.a_generate(\"Hey\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AnswerRelevancyMetric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fa5d393f5d8406fb8b08cfc89b354e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the answer directly addresses the concern about shoe fit, providing a relevant and accurate response.\n"
     ]
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=customLMStudio(),\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=[\"We offer a 40-day full refund at no extra cost\"]\n",
    "\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2075a46c28b64c409f39a8c2aa8f6eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ❌ Answer Relevancy (score: 0.5, threshold: 0.7, strict: False, evaluation model: llama3:8b, reason: The score is 0.50 because the answer includes a statement that is completely off-topic and unrelated to addressing the issue of poorly fitting shoes., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra cost.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['We offer a 40-day full refund at no extra cost']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 0.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=False, metrics_metadata=[MetricMetadata(metric='Answer Relevancy', threshold=0.7, success=False, score=0.5, reason='The score is 0.50 because the answer includes a statement that is completely off-topic and unrelated to addressing the issue of poorly fitting shoes.', strict_mode=False, evaluation_model='llama3:8b', error=None, evaluation_cost=None, verbose_logs='Statements:\\n[\\n    \"We offer a 30-day full refund\",\\n    \"at no extra cost\"\\n]\\n\\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"The statement is irrelevant to the input, which asks about the situation where shoes don\\'t fit.\"\\n    },\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra cost.', expected_output=None, context=None, retrieval_context=['We offer a 40-day full refund at no extra cost'])]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Contextual Precision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The contextual precision metric measures your RAG pipeline's retriever by evaluating whether nodes in your retrieval_context that are relevant to the given input are ranked higher than irrelevant ones. deepeval's contextual precision metric is a self-explaining LLM-Eval, meaning it outputs a reason for its metric score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "03a47dfbee5543f786e2cba7133817bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "69c18cc96da3442d8a4b2cf3396c8927",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the relevant node ('yes' verdict) is correctly ranked as the top node, while the irrelevant nodes ('no' verdicts) are understandably ranked lower due to their lack of useful information in providing a suitable solution for the input 'What if these shoes don't fit?'.\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.7, strict: False, evaluation model: llama3:8b, reason: The score is 1.00 because the retrieval context with a 'yes' verdict clearly provides relevant information about the expected output, while the node ranked second ('no' verdict) is irrelevant and doesn't provide any useful details about refund process or eligibility., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra cost.\n",
      "  - expected output: You are eligible for a 30 day full refund at no extra cost.\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra cost.', '']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics_metadata=[MetricMetadata(metric='Contextual Precision', threshold=0.7, success=True, score=1.0, reason=\"The score is 1.00 because the retrieval context with a 'yes' verdict clearly provides relevant information about the expected output, while the node ranked second ('no' verdict) is irrelevant and doesn't provide any useful details about refund process or eligibility.\", strict_mode=False, evaluation_model='llama3:8b', error=None, evaluation_cost=None, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The context clearly mentions that all customers are eligible for a 30 day full refund, which matches the expected output.\"\\n    },\\n    {\\n        \"verdict\": \"no\",\\n        \"reason\": \"\\'\\' is empty and doesn\\'t provide any useful information about the refund process or eligibility.\"\\n    }\\n]')], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra cost.', expected_output='You are eligible for a 30 day full refund at no extra cost.', context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra cost.', ''])]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\",\"\"]\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=customLMStudio(),    \n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepEvual with OPENAI API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"]=\"your api key\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5e6cdcd47a74c0281cbb0750eab662d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f742bb9db994c85a17269dd5d2d8e9d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the response perfectly addresses the concern raised in the input without any irrelevant statements.\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Answer Relevancy (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4, reason: The score is 1.00 because the response perfectly addressed the concern about the shoes not fitting., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 30-day full refund at no extra cost.\n",
      "  - expected output: None\n",
      "  - context: None\n",
      "  - retrieval context: ['We offer a 40-day full refund at no extra cost']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Answer Relevancy: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics_metadata=[MetricMetadata(metric='Answer Relevancy', threshold=0.7, success=True, score=1.0, reason='The score is 1.00 because the response perfectly addressed the concern about the shoes not fitting.', strict_mode=False, evaluation_model='gpt-4', error=None, evaluation_cost=0.026369999999999998, verbose_logs='Statements:\\n[\\n    \"We offer a 30-day full refund at no extra cost.\"\\n]\\n\\nVerdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": null\\n    }\\n]')], input=\"What if these shoes don't fit?\", actual_output='We offer a 30-day full refund at no extra cost.', expected_output=None, context=None, retrieval_context=['We offer a 40-day full refund at no extra cost'])]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import AnswerRelevancyMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 30-day full refund at no extra cost.\"\n",
    "\n",
    "metric = AnswerRelevancyMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    retrieval_context=[\"We offer a 40-day full refund at no extra cost\"]\n",
    "\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6aaf7b4404ce4d7188e42355bbcc2ff6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Event loop is already running. Applying nest_asyncio patch to allow async execution...\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e83e582ad9c64e929c21b3095d80ee22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "The score is 1.00 because the only node in the retrieval context directly addresses the issue of the shoes not fitting.\n",
      "Evaluating test cases...\n",
      "Event loop is already running. Applying nest_asyncio patch to allow async execution...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "\n",
      "Metrics Summary\n",
      "\n",
      "  - ✅ Contextual Precision (score: 1.0, threshold: 0.7, strict: False, evaluation model: gpt-4, reason: The score is 1.00 because the only node in the retrieval context directly addresses the user's concern about shoes not fitting, hence its high relevance and top ranking., error: None)\n",
      "\n",
      "For test case:\n",
      "\n",
      "  - input: What if these shoes don't fit?\n",
      "  - actual output: We offer a 300-day full refund at no extra cost.\n",
      "  - expected output: You are eligible for a 30 day full refund at no extra cost.\n",
      "  - context: None\n",
      "  - retrieval context: ['All customers are eligible for a 30 day full refund at no extra cost.']\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Overall Metric Pass Rates\n",
      "\n",
      "Contextual Precision: 100.00% pass rate\n",
      "\n",
      "======================================================================\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">✅ Tests finished! Run <span style=\"color: #008000; text-decoration-color: #008000\">\"deepeval login\"</span> to view evaluation results on the web.\n",
       "</pre>\n"
      ],
      "text/plain": [
       "✅ Tests finished! Run \u001b[32m\"deepeval login\"\u001b[0m to view evaluation results on the web.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[TestResult(success=True, metrics_metadata=[MetricMetadata(metric='Contextual Precision', threshold=0.7, success=True, score=1.0, reason=\"The score is 1.00 because the only node in the retrieval context directly addresses the user's concern about shoes not fitting, hence its high relevance and top ranking.\", strict_mode=False, evaluation_model='gpt-4', error=None, evaluation_cost=0.030809999999999997, verbose_logs='Verdicts:\\n[\\n    {\\n        \"verdict\": \"yes\",\\n        \"reason\": \"The text \\'All customers are eligible for a 30 day full refund at no extra cost.\\' directly addresses the concern about shoes not fitting.\"\\n    }\\n]')], input=\"What if these shoes don't fit?\", actual_output='We offer a 300-day full refund at no extra cost.', expected_output='You are eligible for a 30 day full refund at no extra cost.', context=None, retrieval_context=['All customers are eligible for a 30 day full refund at no extra cost.'])]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from deepeval import evaluate\n",
    "from deepeval.metrics import ContextualPrecisionMetric\n",
    "from deepeval.test_case import LLMTestCase\n",
    "\n",
    "# Replace this with the actual output from your LLM application\n",
    "actual_output = \"We offer a 300-day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the expected output from your RAG generator\n",
    "expected_output = \"You are eligible for a 30 day full refund at no extra cost.\"\n",
    "\n",
    "# Replace this with the actual retrieved context from your RAG pipeline\n",
    "retrieval_context = [\"All customers are eligible for a 30 day full refund at no extra cost.\"]\n",
    "\n",
    "metric = ContextualPrecisionMetric(\n",
    "    threshold=0.7,\n",
    "    model=\"gpt-4\",\n",
    "    include_reason=True\n",
    ")\n",
    "test_case = LLMTestCase(\n",
    "    input=\"What if these shoes don't fit?\",\n",
    "    actual_output=actual_output,\n",
    "    expected_output=expected_output,\n",
    "    retrieval_context=retrieval_context\n",
    ")\n",
    "\n",
    "metric.measure(test_case)\n",
    "print(metric.score)\n",
    "print(metric.reason)\n",
    "\n",
    "# or evaluate test cases in bulk\n",
    "evaluate([test_case], [metric])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
